<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>CSML Workshop 2026</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo_dbmml.svg" alt="Workshop logo, depicts a plot of two probability densities." /></span>
						<h1>London Meeting on</h1><h1>Computational Statistics</h1>
						<!--<h3>Monte Carlo Methods &nbsp;â—†&nbsp; Gradient Flows &nbsp;â—†&nbsp; Simulation-Based Inference &nbsp;â—†&nbsp; Variational Inference</h3>-->


						<h3><a href="https://www.ucl.ac.uk">UCL</a> Workshop, <b>28-29 April, 2026.</b></h3>
					
						<nav id='navlink'><h3>  Location: <a href="#location">De Morgan House</a></h3></nav>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#about" class="active">About</a></li>
							<li><a href="#speakers">Speakers</a></li>
							<li><a href="#registration">Registration, Talks and Posters</a></li>
							<li><a href="#schedule">Schedule</a></li>
							<li><a href="#organisers">Organisers</a></li>
							<li><a href="#acknowledgements">Acknowledgements</a></li>
							<li><a href="#location">Location</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						
						<!-- Introduction -->
							<section id="about" class="main">
								<!--This workshop will bring together researchers at the forefront of <b>computational statistics</b> to discuss recent advances in the field. Topics will include <b>Monte Carlo methods</b>, <b>gradient flows</b>, <b>simulation-based inference</b> and <b>variational inference</b>. This workshop is scheduled alongside the <a href="https://imss2026.github.io/">UCL Institute of Mathematics and Statistical Sciences (IMSS) annual lecture.</a></h4>-->
								<h4>
								The London Meeting on Computational Statistics 2026 is a two-day workshop that will bring together researchers at the forefront of computational statistics to discuss recent advances in the field. A broad range of topics will be covered, with a focus on the intersection of computational statistics and machine learning. Examples of topics include (but are not limited to):
								</h4>

								<ul class="topics">
								<li>Monte Carlo methods</li>
								<li>Gradient flows</li>
								<li>Simulation-based inference</li>
								<li>Variational inference</li>
								</ul>

								<h4>
								The workshop is scheduled alongside the
								<a href="https://imss2026.github.io/">UCL Institute of Mathematics and Statistical Sciences (IMSS) Annual Lecture</a>,
								which will take place on 27 April 2026 and will feature Dr Lester Mackey as the keynote speaker.
								</h4>
							</section>

							<!-- Speakers -->
							<section id="speakers" class="main special">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							
								<ul class="features">
									<li>
										<img class="icon major" src="images/arnauddoucet.jpeg" alt="Photo of Arnaud Doucet" width="200" height="200"/>
										<h3><a href="https://www.stats.ox.ac.uk/~doucet/">Arnaud Doucet</a></h3>
										<p>University of Oxford</p>
									</li>

									<li>
										<img class="icon major" src="images/sarahfilippi.jpeg" alt="Photo of Sarah Filippi" width="200" height="200"/>
										<h3><a href="https://profiles.imperial.ac.uk/s.filippi">Sarah Filippi</a></h3>
										<p>Imperial College London</p>
									</li>

									<li>
										<img class="icon major" src="images/mathieugerber.jpeg" alt="Photo of Mathieu Gerber" width="200" height="200"/>
										<h3><a href="https://research-information.bris.ac.uk/en/persons/mathieu-gerber/">Mathieu Gerber</a></h3>
										<p>University of Bristol</p>
									</li>

									<li>
										<img class="icon major" src="images/photo_heishiro.png" alt="Photo of Heishiro" width="200" height="200"/>
										<h3><a href="https://noukoudashisoup.github.io/">Heishiro Kanagawa</a></h3>
										<p>Fujitsu Research</p>
									</li>

									<li>
										<img class="icon major" src="images/nopics.jpg" alt="Photo of no pics" width="200" height="200"/>
										<h3><a href="">TBA</a></h3>
										<p>TB A</p>
									</li>

									<li>
										<img class="icon major" src="images/gilleslouppe.jpg" alt="Photo of Gilles Louppe" width="200" height="200"/>
										<h3><a href="https://glouppe.github.io/">Gilles Louppe</a></h3>
										<p>University of LiÃ¨ge</p>
									</li>

									<li>
										<img class="icon major" src="images/Arina.jpg" alt="Photo of Arina Odnoblyudova" width="200" height="200"/>
										<h3><a href="https://jularina.github.io/arina-odv/">Arina Odnoblyudova</a></h3>
										<p>University College London</p>
									</li>

									<li>
										<img class="icon major" src="images/dennisprangle.jpeg" alt="Photo of Dennis Prangle" width="200" height="200"/>
										<h3><a href="https://dennisprangle.github.io/">Dennis Prangle</a></h3>
										<p>University of Bristol</p>
									</li>

									<li>
										<img class="icon major" src="images/marinariabiz.jpeg" alt="Photo of Marina Riabiz" width="200" height="200"/>
										<h3><a href="https://www.kcl.ac.uk/people/marina-riabiz">Marina Riabiz</a></h3>
										<p>King's College London</p>
									</li>
								
								</ul>
							</section>
						

							<section id="registration" class="main special">
							<header class="major">
									<h2>Registration, Talks and Posters</h2>
							</header>
							<h3><b>Registration</b></h3>
							<h4> The cost is <b> Â£30</b>. Please register through <a href="https://onlinestore.ucl.ac.uk/conferences-and-events/faculty-of-mathematical-physical-sciences-c06/department-of-statistical-science-f61/f61-london-meeting-on-computational-statistics-2026">this link</a>. Coffee and lunch will be provided!</h4><br> 
							
							<h3><b>Contributed talks & posters</b></h3>
							<h4> We welcome contributions on monte carlo methods, simulation-based inference, gradient flows, and variational inference.</h4>
							<h4><a class="btn btn-secondary" href="https://forms.microsoft.com/Pages/ResponsePage.aspx?id=_oivH5ipW0yTySEKEdmlwpZA9mVzyBREiJcV5SapJ05UMjdTVENKUktJU0ZPSTJGVVFMNFM1VVFENi4u">Submit a talk or poster.</a></h4>
							
							<h4>Deadline: <b>15th March 5pm GMT.</b></h4>
							</section>


							<section id="schedule" class="main special">
								<!--<span class="image main"><img src="images/portico2.png" alt="Photo of the main campus of University College London, with the university logo in the top right corner." /></span>-->
								<header class="major">
									<h2>Schedule</h2>
								</header>


								<h3><b>Tuesday, April 28th 2026</b></h3>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width=15%>9:00â€“9:30</td>
												<td align="left">Registration</td>
											</tr>
											<tr>
												<td>9:30â€“9:45</td>
												<td align="left">ðŸ‘‹ Welcome from the organisers</td>
											</tr>
											<tr>
												<td>9:45â€“10:15</td>
												<td align="left"><em>Title: Adaptive model size in Gaussian processes for continual learning</em>
												<br><b>Sarah Filippi</b> (Imperial College London)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														Many machine learning models require fixing their capacity before training, such as the number of neurons in a neural network or inducing points in a Gaussian process. Increasing capacity typically improves performance until the available information in the data is captured, after which computational costs continue to grow without benefit. This raises the question: how big is big enough? In this talk, I address this problem for Gaussian processes in continual learning, where data arrives incrementally and the final dataset size is unknown in advance. In this setting, standard heuristics for choosing a fixed model size are unavailable. I present a method that automatically adapts model capacity online, achieving near-optimal predictive performance while avoiding unnecessary computation. All hyperparameters are set without access to dataset properties, and experiments across diverse datasets show that the method performs robustly with substantially less tuning than existing approaches. I will also briefly discuss related issues in scaling Gaussian processes with stochastic optimization, showing how minibatch-induced gradient noise can slow convergence in sparse variational models, and outlining a variance-reduction approach that accelerates training without full-batch computations.
													</div>
												</td>

											</tr>
											
											<tr>
											
												<td>10:15â€“10:45</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>Contributed Talk</b>
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>

											</tr>
											<tr>
												<td>10:45â€“11:30</td>
												<td align="left">â˜• Coffee break</td>
											</tr>
											<tr>
												<td>11:30â€“12:00</td>
												<td align="left"><em>Title: Minimum distance summaries for robust neural posterior estimation</em>
												<br><b>Dennis Prangle</b> (University of Bristol)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														Neural posterior estimation enables approximate Bayesian inference using conditional density estimation from simulated prior-data pairs, typically reducing the data to low-dimensional summary statistics. NPE is susceptible to misspecification when observations deviate from the training distribution. We introduce minimum-distance summaries, a plug-in robust NPE method that adapts queried test-time summaries independently of the pretrained NPE. Leveraging the maximum mean discrepancy (MMD) as a distance between observed data and a summary-conditional predictive distribution, the adapted summary inherits strong robustness properties from the MMD. We demonstrate that the algorithm can be implemented efficiently with random Fourier feature approximations, yielding a lightweight, model-free test-time adaptation procedure. We provide theoretical guarantees for the robustness and consistency of our algorithm and empirically evaluate it on a range of synthetic and real-world tasks, demonstrating substantial robustness gains with minimal additional overhead.
													</div>
												</td>
												
											</tr>
											<tr>
												<td>12:00â€“12:30</td>
												<td align="left"><em>Title: A computationally-tractable measure of global sensitivity for Bayesian inference</em>
												<br><b>Arina Odnoblyudova</b> (University College London)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														Bayesian inference should ideally not be overly sensitive to the choice of prior or hyperparameters, but even defining and measuring this sensitivity is challenging.  Existing global sensitivity measures typically involve significant trade-offs between strength of the measure, interpretability, and computational tractability. Unfortunately, most methods are unable to serve the needs of modern Bayesian inference due to their high computational cost and poor performance in multiple dimensions. To address these limitations, we introduce a new approach to global sensitivity analysis which only requires a set of samples from a reference posterior and the ability to evaluate score functions, making it broadly computationally tractable. We demonstrate our proposed method on challenging Bayesian inference problems which are practically out of reach of existing approaches, including Bayesian inference for heavy-tailed time series, simulation-based inference for problems in telecommunications engineering, and generalised Bayesian inference for doubly-intractable models.
													</div>
												</td>

												
											</tr>

											<tr>
												<td>12:30â€“13:45</td>
												<td align="left">ðŸ¥— Lunch</td>
											</tr>
											<tr>
												<td>13:45â€“14:15</td>
												<td align="left"><em>Title: Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both</em>
												<br><b>Mathieu Gerber</b> (University of Bristol)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable andwhose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.
													</div>
												</td>
											</tr>
											<tr>
												<td>14:15â€“14:45</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>Marina Riabiz</b> (King's College London)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>
											</tr>
											
											<tr>
												<td>14:45â€“15:30</td>
												<td align="left">â˜• Coffee break</td>
											</tr>
											<tr>
												<td>15:30â€“16:00</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>Contributed Talk</b>
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>
											</tr>
											<tr>
												<td>16:00â€“16:30</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>Contributed Talk</b>
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>

											</tr>
							
										</tbody>
									</table>
								</div>
								<br>
								<h3><b>Wednesday, April 29th 2026</b></h3>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width=15%>9:00â€“9:30</td>
												<td align="left">â˜• Morning coffee</td>
											</tr>
											<tr>
												<td>9:30â€“10:00</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>TBA</b> (TBA)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>
											</tr>
											<tr>
												<td>10:00â€“10:30</td>
												<td align="left"><em>Title: A computable measure of suboptimality for entropy-regularised variational objectives</em>
												<br><b>Heishiro Kanagawa</b> (Fujitsu Research)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called gradient discrepancy, and in particular a kernel gradient discrepancy (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel characterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained.
													</div>
												</td>
											</tr>
											<tr>
												<td>10:30â€“11:15</td>
												<td align="left">â˜• Coffee break</td>
											</tr>
											<tr>
												<td>11:15â€“11:45</td>
												<td align="left"><em>Title: Self-speculative masked diffusions</em>
												<br><b>Arnaud Doucet</b> (University of Oxford)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														We present self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled; however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. We reduce the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. We find that we can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models.
													</div>
												</td>
											</tr>
											<tr>
												<td>11:45â€“12:15</td>
												<td align="left"><em>Title: Scaling-up simulation-based inference with diffusion models</em>
												<br><b>Gilles Louppe</b> (University of LiÃ¨ge)
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														Deep generative models are transforming how we solve some of science's hardest puzzles: inverse problems where we must work backwards from noisy, incomplete observations to uncover hidden physical states. In this talk, we will explore three scales of application, from characterizing the atmospheres of distant exoplanets light years away, to reconstructing turbulent fluid dynamics from sparse measurements, to assimilating satellite data across the entire Earth's atmosphere in real time. We will see how normalizing flows, score based diffusion models, and latent space compression allow us to tackle problems spanning tens to billions of variables, revealing not just single solutions but entire distributions of physically plausible states.
													</div>
												</td>
											</tr>
											<tr>
												<td>12:15â€“13:15</td>
												<td align="left">ðŸ¥— Lunch</td>
											</tr>
											<tr>
												<td>13:15â€“14:45</td>
												<td align="left">ðŸª§ Poster session</td>
											</tr>
											<tr>
												<td>14:45â€“15:30</td>
												<td align="left">â˜• Coffee break</td>
											</tr>
											<tr>
												<td>15:30â€“16:00</td>
												<td align="left"><em>Title: TBA</em>
												<br><b>Contributed Talk</b>
												<br><a class="collapsible" href="javascript:void(0)">[show abstract]</a>
													<div class="content">
														TBA
													</div>
												</td>
											</tr>
											<tr>
												<td>16:00â€“16:15</td>
												<td align="left">ðŸ‘‹ Closing remarks</td>
											</tr>



											
										</tbody>
									</table>
								</div>
							</section>

						
						
						<!-- Schedule -->
							<section id="organisers" class="main special">
								<header class="major">
									<h2>Organisers</h2>
								</header>
								<h3>To learn more about our organisers, see the <a href="https://fsml-ucl.github.io/"> FSML research group </a> webpage!</h3>								<ul class="features">
									<li>
										<img class="icon major" src="images/harita.jpg" alt="Photo of Harita" width="200" height="200"/>
										<h3><a href="https://haritadell.github.io/">Harita Dellaporta</a></h3>
										<h4><b>Lead organiser</b></h4>
									</li>

									<li>
										<img class="icon major" src="images/fx.png" alt="Photo of FranÃ§ois-Xavier Briol" width="200" height="200"/>
										<h3><a href="https://fxbriol.github.io/">FranÃ§ois-Xavier Briol</a></h3>
										<h4>Co-organiser</h4>
									</li>

								</ul>
									<ul class="features">
									<li>
										<img class="icon major" src="images/hudson.jpg" alt="Photo of Hudson" width="200" height="200"/>
										<h3><a href="https://hudsonchen.github.io/">Zonghao (Hudson) Chen</a></h3>
										<h4>Co-organiser</h4>
									</li>
									
									<li>
										<img class="icon major" src="images/will.jpg" alt="Photo of Will" width="200" height="200"/>
										<h3><a href="https://williamlaplante.github.io/">William Laplante</a></h3>
										<h4>Co-organiser</h4>
									</li>
								</ul>
								<!--<p>We are very grateful for funding from the <a href="https://www.grad.ucl.ac.uk/funds/ucl-fellowship-incubator-awards.html">UCL Fellowship Incubator Fund</a>, the <a href="https://www.ucl.ac.uk/statistics/department-statistical-science">UCL Department of Statistical Science</a>, the <a href="https://www.ucl.ac.uk/mathematical-statistical-sciences/institute-mathematical-and-statistical-sciences">UCL Institute for Mathematical and Statistical Sciences (IMSS)</a>, and <a href="https://www.ucl.ac.uk/foundational-ai-cdt/foundational-artificial-intelligence-mphilphd">UKRI CDT in Foundational AI</a> funded by the Engineering and Physical Sciences Research Council [EP/S021566/1].</p>-->
							</section>

						<!--Acknowledgements-->
						<section id="acknowledgements" class="main special">
								<header class="major">
									<h2>Acknowledgements</h2>
								</header>
							<h4>
								This workshop was supported financially through the EPSRC grant "Transfer Learning for Monte Carlo Methods" (EP/Y022300/1) and the UCL department of Statistical Science's section on <a href="https://www.ucl.ac.uk/mathematical-physical-sciences/statistics/research/computational-statistics-and-machine-learning">Computational Statistics and Machine Learning</a>. The organisers are also particularly grateful to the <a href="https://ucl-ellis.github.io/"> ELLIS Unit London at UCL</a> (co-organisers) and the <a href="https://rss.org.uk/membership/rss-groups-and-committees/sections/statistical-computing/">Royal Statistical Society's section on Computational Statistics and Machine Learning</a> for supporting this event.
							</h4>

							<div style="display:flex; justify-content:center; gap:12px; margin-top:10px;">
							<img
							src="images/ucl.jpg"
							alt="UCL logo"
							style="width:150px; height:150px; object-fit:contain; background:#fff;" 
							/>
							<img
							src="images/ellis-logo.png"
							alt="Ellis logo"
							style="width:150px; height:150px; object-fit:contain; background:#fff;" 
							/>
							<img
							src="images/csml_logo_vector.png"
							alt="CSML logo"
							style="width:300px; height:150px; object-fit:contain; background:#fff;" 
							/>
							<img
							src="images/RSS.jpg"
							alt="RSS logo"
							style="width:300px; height:150px; object-fit:contain; background:#fff;" 
							/>
							</div>
						</section>

						<!-- Location -->
							<section id="location" class="main special">
								<header class="major">
									<h2>Location</h2>
								</header>
								<p>The workshop will be hosted at the London Mathematical Society in Central London.<br> The address is De Morgan House, 57-58 Russell Sq, London WC1B 4HS.</p>
								<a href="https://maps.app.goo.gl/GKckuPXGrX6NEroB7">Google Maps</a> </p>
							</section>
							<!--<section>
								<span class="image main"><img src="images/bentham_house.jpeg" alt="Photo of Bentham house, the venue of the workshop." /></span>
							</section>-->
				</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; DBMML workshop. Design by <a href="https://html5up.net">HTML5UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
